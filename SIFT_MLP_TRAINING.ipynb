{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SIFT_MLP_TRAINING.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOCQJthx/KlR22DxbYIQuSs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a6r_A6ceyjmu","executionInfo":{"status":"ok","timestamp":1619923686970,"user_tz":-60,"elapsed":2092,"user":{"displayName":"Arman Sarjou","photoUrl":"","userId":"03844234635043925964"}},"outputId":"305e6f62-9719-4886-c008-4a7237a0ea59"},"source":["import torch\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Assuming that we are on a CUDA machine, this should print a CUDA device:\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bSiizkJHyveq","executionInfo":{"status":"ok","timestamp":1619923687624,"user_tz":-60,"elapsed":2722,"user":{"displayName":"Arman Sarjou","photoUrl":"","userId":"03844234635043925964"}},"outputId":"c1cda30e-e39e-42cc-dc5b-d406a370cd68"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZPfK5JOyyoM","executionInfo":{"status":"ok","timestamp":1619923687625,"user_tz":-60,"elapsed":2706,"user":{"displayName":"Arman Sarjou","photoUrl":"","userId":"03844234635043925964"}},"outputId":"de9a8bcc-11d9-4c8c-bf7b-257436ee209c"},"source":["import os\n","\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/Computer Vision/Coursework/' \n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['CW_Dataset-2.zip', 'Untitled0.ipynb', 'EmotionRecognition.py', '__pycache__', 'ASARJOU_CV_VIDEO.avi', 'ASARJOU_CV_VIDEO-3.avi', 'ASARJOU_CV_CW_SHORT.mp4', 'ASARJOU_CV_CW_SHORT.avi', 'train_lab.csv', 'val_lab.csv', 'fin_MLP_ADASYN.pkl', 'fin_CNN.pkl', 'fin_SIFT_MLP.pkl']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1K_n2ICDglcuJFjeyI1Rb-ZIwXkV073Lc"},"id":"hmUKgyZvy756","executionInfo":{"status":"ok","timestamp":1619923706239,"user_tz":-60,"elapsed":21304,"user":{"displayName":"Arman Sarjou","photoUrl":"","userId":"03844234635043925964"}},"outputId":"111bec4f-de9f-4f94-d554-478e238b38e3"},"source":["# FROM LAB 9 #\n","# Identify path to zipped dataset\n","zip_path = os.path.join(GOOGLE_DRIVE_PATH, 'CW_Dataset-2.zip')\n","\n","# Copy it to Colab\n","!cp '{zip_path}' .\n","\n","# Unzip it (removing useless files stored in the zip)\n","!yes|unzip -q CW_Dataset-2.zip\n","\n","# Delete zipped version from Colab (not from Drive)\n","!rm CW_Dataset-2.zip"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UTn9FFM9y906","executionInfo":{"status":"ok","timestamp":1619923706776,"user_tz":-60,"elapsed":21833,"user":{"displayName":"Arman Sarjou","photoUrl":"","userId":"03844234635043925964"}},"outputId":"81909450-a554-4692-f9c6-0ad38e5cb36a"},"source":["!pip install opencv-python==4.4.0.46"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: opencv-python==4.4.0.46 in /usr/local/lib/python3.7/dist-packages (4.4.0.46)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.4.0.46) (1.19.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xmfp0YJzzCs0"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import cv2\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torchvision.io import read_image\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","from torchvision.transforms import ToTensor, Lambda\n","import time\n","import pandas as pd\n","import copy\n","from PIL import Image\n","from skimage import io, img_as_ubyte\n","\n","\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DlIZsCjpzIkj"},"source":["class CustomImageDataset(Dataset):\n","\n","    def __init__(self, annotations_df, img_dir, transform=None, target_transform=None):\n","\n","        self.img_labels = annotations_df\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = io.imread(img_path)\n","        h = image.copy()\n","\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","      \n","        sample = {\"image\": image, \"label\": label}\n","        return image, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCeFL_RhzPgI"},"source":["class MLP(nn.Module):\n","    def __init__(\n","            self,\n","            hidden_num = 40,\n","            dropout = 0.1,\n","            nonlin = torch.nn.Sigmoid()\n","\n","    \n","    ):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(70,hidden_num)\n","        self.fc2 = nn.Linear(hidden_num,8)\n","        self.non_lin = nonlin\n","        self.dropout = dropout\n","        \n","    def forward(self, x):\n","        hidden = self.fc1(x)\n","        hidden = self.non_lin(hidden)\n","        output = self.fc2(hidden)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-VchfqqzTkT"},"source":["data_dir = \"CW_Dataset-2\"\n","l_test = os.path.join(data_dir, \"labels\", \"list_label_test.txt\")\n","d_test = os.path.join(data_dir, \"test\")\n","d = os.path.join(data_dir, \"train\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KfPqnFNNzYkT"},"source":["train_lab = pd.read_csv(os.path.join(GOOGLE_DRIVE_PATH, 'train_lab.csv'))\n","train_lab = train_lab.drop(columns='Unnamed: 0')\n","val_lab = pd.read_csv(os.path.join(GOOGLE_DRIVE_PATH, 'val_lab.csv'))\n","val_lab = val_lab.drop(columns='Unnamed: 0')\n","test_lab = pd.read_csv(l_test, delimiter= ' ',header=None)\n","test_lab = test_lab.rename(columns={0: \"name\", 1: \"lbl\"})\n","test_lab['name'] = test_lab['name'].str[:-4] + \"_aligned\" + \".jpg\"\n","test_lab['lbl'] = test_lab['lbl'] - 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z2kVL9b2zgBV"},"source":["data_means = [0.485, 0.456, 0.406]\n","data_stds = [0.229, 0.224, 0.225]\n","\n","training_data = CustomImageDataset(train_lab, d, transform=transforms.Compose([transforms.ToPILImage(),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomGrayscale(),\n","        transforms.RandomRotation(10),\n","        transforms.RandomVerticalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(data_means, data_stds)]))\n","\n","validation_data = CustomImageDataset(val_lab, d, transform=transforms.Compose([transforms.ToPILImage(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(data_means, data_stds)]))\n","\n","test_data = CustomImageDataset(test_lab, d_test, transform=transforms.Compose([transforms.ToPILImage(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(data_means, data_stds)]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0EYYDk6Vziy5"},"source":["ds_train = DataLoader(training_data, batch_size =4, shuffle=True)\n","ds_val = DataLoader(validation_data, batch_size =4, shuffle=True)\n","ds_test = DataLoader(test_data, batch_size =4, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K_hluXWgzl_i"},"source":["### ADAPTED FROM LAB 8 (LAB IMPLEMENTATION OF SIFT BOVW) ###\n","\n","def sift_conv(ds, b_num):\n","  des_list = []\n","  l_set = []\n","  for image, label in ds: #taking images and applying sift \n","    for i in range(b_num-1):\n","      #print(image[i].shape)\n","      inp = image[i].numpy().transpose((1, 2, 0))\n","      image8bit = cv2.normalize(inp, None, 0, 255, cv2.NORM_MINMAX).astype('uint8')\n","      sift = cv2.SIFT_create()\n","      SIFT_kp, SIFT_des = sift.detectAndCompute(image8bit, None)\n","      if SIFT_des is not None:\n","          des_list.append(SIFT_des)\n","          l_set.append(label[i])\n","  des_array = np.vstack(des_list)\n","  return des_array, des_list, l_set"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iIr7jvImzvao"},"source":["train_arr, train_list, train_labels = sift_conv(ds_train, 4)\n","val_arr, val_list, val_labels = sift_conv(ds_val, 4)\n","test_arr, test_list, test_labels = sift_conv(ds_test, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXYxqaODzyAI"},"source":["### ADAPTED FROM LAB 8 (LAB IMPLEMENTATION OF SIFT BOVW) ###\n","\n","from sklearn.cluster import MiniBatchKMeans\n","def kmeans_bovw(des_array, des_list):\n","  batch_size = des_array.shape[0] // 4\n","  kmeans = MiniBatchKMeans(n_clusters=70, batch_size=batch_size).fit(des_array) #Doing KMeans for BoVW\n","\n","  # Convert descriptors into histograms of codewords for each image\n","  hist_list = []\n","  idx_list = []\n","\n","  for des in des_list:\n","      hist = np.zeros(70)\n","\n","      idx = kmeans.predict(des)\n","      idx_list.append(idx)\n","      for j in idx:\n","          hist[j] = hist[j] + (1 / len(des))\n","      hist_list.append(hist)\n","\n","  hist_array = np.vstack(hist_list)\n","  return hist_array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SJJzPuFz0ly"},"source":["train_bovw = kmeans_bovw(train_arr, train_list)\n","val_bovw = kmeans_bovw(val_arr, val_list)\n","test_bovw = kmeans_bovw(test_arr, test_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3R0Mo1mz2ym"},"source":["from torch.utils.data import TensorDataset\n","train_ds = TensorDataset(torch.tensor(train_bovw), torch.tensor(train_labels))\n","val_ds = TensorDataset(torch.tensor(val_bovw), torch.tensor(val_labels))\n","\n","train_dl = DataLoader(train_ds, batch_size=4)\n","val_dl = DataLoader(val_ds, batch_size = 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j7C5tF9C0E5X"},"source":["training_dataset_sizes = len(train_labels)\n","validation_dataset_sizes = len(val_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJFWivYF0LJa"},"source":["### ADDAPTED FROM LAB 9 ###\n","def train_model(model, criterion, optimizer, num_epochs=10):\n","    since = time.time()\n","\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        \n","        print(f\"Epoch {epoch}/{num_epochs - 1}\")\n","        print('----------------')\n","\n","        # Each epoch has a training and validation phase\n","        \n","        model.train()  # Set model to training mode\n","        running_loss = 0.0\n","        running_corrects = 0\n","\n","            # Iterate over data\n","        for inputs, labels in train_dl:\n","       \n","                # move data to GPU\n","                input = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(True):\n","           \n","                    outputs = model(input.float())\n","                    _, preds = torch.max(outputs, 1)\n","      \n","                    \n","                    loss = criterion(outputs, labels)\n","             \n","                    # backward + optimize only if in training phase\n","\n","                    loss.backward()\n","                    optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * input.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            # update learning rate with scheduler\n","\n","\n","        epoch_loss = running_loss / training_dataset_sizes\n","        epoch_acc = running_corrects.double() / training_dataset_sizes\n","\n","        print(f\"train loss: {epoch_loss:.4f} train acc: {epoch_acc:.4f}\")\n","      \n","        \n","        model.eval()  # Set model to evaluation mode\n","        running_loss = 0.0\n","        running_corrects = 0\n","\n","        for inputs_val, labels_val, in val_dl:\n","                # move data to GPU\n","                input_val = inputs_val.to(device)\n","                labels_val = labels_val.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(False):\n","                    outputs_val = model(input_val.float())\n","               \n","                    _, preds_val = torch.max(outputs_val, 1)\n","                    loss_val = criterion(outputs_val, labels_val)\n","\n","                # statistics\n","                running_loss += loss_val.item() * input_val.size(0)\n","                running_corrects += torch.sum(preds_val == labels_val.data)\n","\n","\n","        epoch_loss = running_loss / validation_dataset_sizes\n","        epoch_acc = running_corrects.double() / validation_dataset_sizes\n","\n","        print(f\"val loss: {epoch_loss:.4f} val acc: {epoch_acc:.4f}\")\n","\n","\n","\n","    time_elapsed = time.time() - since\n","\n","    print(time_elapsed)\n","    torch.save(model,os.path.join(GOOGLE_DRIVE_PATH,'fin_SIFT_MLP.pth'))\n","    return model, best_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fTt9deAv0eBW","executionInfo":{"status":"ok","timestamp":1619927613634,"user_tz":-60,"elapsed":70211,"user":{"displayName":"Arman Sarjou","photoUrl":"","userId":"03844234635043925964"}},"outputId":"fc925a98-987d-44ea-9d3c-6837078cf8ed"},"source":["model = MLP()\n","\n","model = model.to(device)\n","    \n","    # Define criterion\n","criterion = nn.CrossEntropyLoss()\n","    \n","    # optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.0008697902379323465)\n","    \n","    # Train model\n","best_model= train_model(model, criterion, optimizer, num_epochs=20)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0/19\n","----------------\n","train loss: 1.6593 train acc: 0.3781\n","val loss: 1.6770 val acc: 0.3829\n","Epoch 1/19\n","----------------\n","train loss: 1.6386 train acc: 0.3861\n","val loss: 1.6768 val acc: 0.3829\n","Epoch 2/19\n","----------------\n","train loss: 1.6338 train acc: 0.3861\n","val loss: 1.6772 val acc: 0.3829\n","Epoch 3/19\n","----------------\n","train loss: 1.6278 train acc: 0.3860\n","val loss: 1.6780 val acc: 0.3829\n","Epoch 4/19\n","----------------\n","train loss: 1.6205 train acc: 0.3867\n","val loss: 1.6795 val acc: 0.3829\n","Epoch 5/19\n","----------------\n","train loss: 1.6122 train acc: 0.3891\n","val loss: 1.6818 val acc: 0.3835\n","Epoch 6/19\n","----------------\n","train loss: 1.6036 train acc: 0.3913\n","val loss: 1.6850 val acc: 0.3851\n","Epoch 7/19\n","----------------\n","train loss: 1.5952 train acc: 0.3984\n","val loss: 1.6890 val acc: 0.3819\n","Epoch 8/19\n","----------------\n","train loss: 1.5877 train acc: 0.4030\n","val loss: 1.6937 val acc: 0.3813\n","Epoch 9/19\n","----------------\n","train loss: 1.5809 train acc: 0.4064\n","val loss: 1.6988 val acc: 0.3802\n","Epoch 10/19\n","----------------\n","train loss: 1.5750 train acc: 0.4080\n","val loss: 1.7040 val acc: 0.3819\n","Epoch 11/19\n","----------------\n","train loss: 1.5698 train acc: 0.4091\n","val loss: 1.7093 val acc: 0.3764\n","Epoch 12/19\n","----------------\n","train loss: 1.5651 train acc: 0.4124\n","val loss: 1.7145 val acc: 0.3732\n","Epoch 13/19\n","----------------\n","train loss: 1.5610 train acc: 0.4149\n","val loss: 1.7196 val acc: 0.3743\n","Epoch 14/19\n","----------------\n","train loss: 1.5574 train acc: 0.4166\n","val loss: 1.7245 val acc: 0.3732\n","Epoch 15/19\n","----------------\n","train loss: 1.5541 train acc: 0.4163\n","val loss: 1.7293 val acc: 0.3726\n","Epoch 16/19\n","----------------\n","train loss: 1.5513 train acc: 0.4167\n","val loss: 1.7338 val acc: 0.3715\n","Epoch 17/19\n","----------------\n","train loss: 1.5487 train acc: 0.4183\n","val loss: 1.7381 val acc: 0.3721\n","Epoch 18/19\n","----------------\n","train loss: 1.5465 train acc: 0.4181\n","val loss: 1.7422 val acc: 0.3705\n","Epoch 19/19\n","----------------\n","train loss: 1.5444 train acc: 0.4178\n","val loss: 1.7461 val acc: 0.3726\n","69.6279366016388\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z0QzWm-69mxJ"},"source":["model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v_Ywcnrc-YUQ"},"source":[""],"execution_count":null,"outputs":[]}]}